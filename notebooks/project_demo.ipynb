{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "McOT7TvONmnm"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/my_library.zip -d /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XOR Neural Network Example\n",
        "\n",
        "This text demonstrates a simple neural network built from scratch to solve the XOR problem using NumPy. The network has a 2-2-1 architecture, meaning it has 2 input neurons, 2 neurons in a hidden layer, and 1 output neuron.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "We use the classic XOR truth table as the dataset. The network should learn to output 1 when the inputs differ and 0 when they are the same.\n",
        "\n",
        "## Network Components\n",
        "\n",
        "Dense Layer: Fully connected layer handling linear transformations.\n",
        "\n",
        "Activation Function (Sigmoid): Introduces non-linearity, mapping outputs between 0 and 1.\n",
        "\n",
        "Loss Function (MSE): Mean Squared Error, measuring the difference between predicted and true outputs.\n",
        "\n",
        "Optimizer (SGD): Stochastic Gradient Descent, updating weights based on computed gradients.\n",
        "\n",
        "Sequential Model: Container that chains layers together, handling forward and backward passes.\n",
        "\n",
        "## Training Process\n",
        "\n",
        "The network trains for multiple epochs. Each epoch performs:\n",
        "\n",
        "**Forward pass:** Compute the network’s prediction.\n",
        "\n",
        "**Loss calculation:** Measure how far predictions are from true outputs.\n",
        "\n",
        "**Backward pass:** Compute gradients via backpropagation.\n",
        "\n",
        "**Weight update:** Adjust weights using the optimizer.\n",
        "\n",
        "## Expected Output\n",
        "\n",
        "The training prints the loss every 1000 epochs.\n",
        "\n",
        "After training, predictions are shown for all XOR inputs.\n",
        "\n",
        "**Note:** In this configuration, the network may not successfully learn XOR because a single hidden layer with 2 neurons and sigmoid activation is often insufficient. Predictions tend to be around 0.5 for all inputs.\n",
        "\n",
        "## Classes and Functions\n",
        "\n",
        "**Dense:** Fully connected layer.\n",
        "\n",
        "**Sigmoid / Tanh:** Activation function.\n",
        "\n",
        "**MSE:** Mean Squared Error loss.\n",
        "\n",
        "**SGD:** Optimizer using gradient descent.\n",
        "\n",
        "**Sequential:** Handles forward and backward passes for all layers.\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook demonstrates the complete workflow of building and training a neural network from scratch: forward pass → loss computation → backward pass → weight update. It highlights the limitations of small networks and the importance of proper architecture for solving non-linear problems like XOR.\n",
        "\n",
        "learning rate:"
      ],
      "metadata": {
        "id": "8exew8vD7ar2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# user choices:\n",
        "\n",
        "\n",
        "1st layer i/ps: 2\n",
        "\n",
        "1st layer o/ps: 2\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "2nd layer i/ps: 2\n",
        "\n",
        "2nd layer o/ps: 1\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "activation function : sigmoid, for both layers.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**learning rate**: 0.5\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "no. of iterations : 10\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "iJ2T4quCFmKD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BG90Q7AHOYCk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3372fc03-c3c0-438b-cc68-0f393002b74f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss=0.25000895005801566\n",
            "Epoch 1000, loss=0.25000000002035444\n",
            "Epoch 2000, loss=0.25000000002026423\n",
            "Epoch 3000, loss=0.250000000020174\n",
            "Epoch 4000, loss=0.2500000000200839\n",
            "Epoch 5000, loss=0.250000000019994\n",
            "Epoch 6000, loss=0.25000000001990424\n",
            "Epoch 7000, loss=0.25000000001981454\n",
            "Epoch 8000, loss=0.25000000001972505\n",
            "Epoch 9000, loss=0.2500000000196356\n",
            "Predictions:\n",
            " [[0.50000409]\n",
            " [0.49999935]\n",
            " [0.50000066]\n",
            " [0.49999591]]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('/content/my_library')\n",
        "import numpy as np\n",
        "from layers import Dense\n",
        "from activations import Sigmoid\n",
        "from losses import MSE\n",
        "from optimizers import SGD\n",
        "from model import Sequential\n",
        "\n",
        "# XOR dataset\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Create tiny network: 2 inputs -> 2 hidden -> 1 output\n",
        "layers = [\n",
        "    Dense(2, 2, activation=Sigmoid()),\n",
        "    Dense(2, 1, activation=Sigmoid())\n",
        "]\n",
        "\n",
        "model = Sequential(layers)\n",
        "loss_fn = MSE()\n",
        "optimizer = SGD(lr=0.5)\n",
        "\n",
        "# Training loop (just a few epochs)\n",
        "for epoch in range(10000):\n",
        "    # Forward\n",
        "    out = model.forward(X)\n",
        "    loss = loss_fn.forward(out, y)\n",
        "\n",
        "    # Backward\n",
        "    dA = loss_fn.backward()\n",
        "    model.backward(dA)\n",
        "\n",
        "    # Update weights\n",
        "    for layer in layers:\n",
        "        optimizer.step(layer)\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, loss={loss}\")\n",
        "\n",
        "# Test predictions\n",
        "pred = model.forward(X)\n",
        "print(\"Predictions:\\n\", pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradiant check ( Analytical vs Numerical):\n",
        "For analytical the formula is: ∂L/∂W ≈ [L(W + ε) - L(W - ε)]/(2ε)\n",
        "\n"
      ],
      "metadata": {
        "id": "EYD3IJo5IFJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add analytical gradiant descent"
      ],
      "metadata": {
        "id": "nOBd7Sa5Iryk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# user choices2:\n",
        "\n",
        "\n",
        "1st layer i/ps: 2\n",
        "\n",
        "1st layer o/ps: 4\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "2nd layer i/ps: 4\n",
        "\n",
        "2nd layer o/ps: 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "activation function: tanh for first layer and sigmoid for second layer.\n",
        "\n",
        "\n",
        "---\n",
        "starting from 0.1, lowering for stability until it ouutputs the correct results..\n",
        "\n",
        "**learning rate**: 0.075\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "no. of iterations : 10\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TDvkZclBCwsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/my_library')\n",
        "import numpy as np\n",
        "from layers import Dense\n",
        "from activations import Sigmoid, Tanh\n",
        "from losses import MSE\n",
        "from optimizers import SGD\n",
        "from model import Sequential\n",
        "\n",
        "# XOR dataset\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Create network: 2 inputs -> 4 hidden -> 1 output\n",
        "# Hidden layer uses Tanh, output layer uses Sigmoid\n",
        "layers = [\n",
        "    Dense(2, 4, activation=Tanh()),      # Hidden layer\n",
        "    Dense(4, 1, activation=Sigmoid())    # Output layer\n",
        "]\n",
        "\n",
        "model = Sequential(layers)\n",
        "loss_fn = MSE()\n",
        "optimizer = SGD(lr=0.075)  # lower learning rate for stability with Tanh\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    out = model.forward(X)\n",
        "    loss = loss_fn.forward(out, y)\n",
        "\n",
        "    # Backward pass\n",
        "    dA = loss_fn.backward()\n",
        "    model.backward(dA)\n",
        "\n",
        "    # Update weights\n",
        "    for layer in layers:\n",
        "        optimizer.step(layer)\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, loss={loss}\")\n",
        "\n",
        "# Test predictions\n",
        "pred = model.forward(X)\n",
        "print(\"\\nPredictions:\\n\", pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIIHfaW3Ac9-",
        "outputId": "5b2822e3-7fae-46c1-c4da-49fe2f86481c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss=0.250000001070065\n",
            "Epoch 1000, loss=0.2500000008247674\n",
            "Epoch 2000, loss=0.25000000082052476\n",
            "Epoch 3000, loss=0.25000000081630586\n",
            "Epoch 4000, loss=0.25000000081211066\n",
            "Epoch 5000, loss=0.2500000008079389\n",
            "Epoch 6000, loss=0.25000000080379037\n",
            "Epoch 7000, loss=0.2500000007996649\n",
            "Epoch 8000, loss=0.2500000007955622\n",
            "Epoch 9000, loss=0.2500000007914822\n",
            "\n",
            "Predictions:\n",
            " [[0.50001509]\n",
            " [0.49997948]\n",
            " [0.50002053]\n",
            " [0.49998492]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using tensorFlow"
      ],
      "metadata": {
        "id": "uamFkAgQJsoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tensorflow for xor"
      ],
      "metadata": {
        "id": "INFbn_36J2-i"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}